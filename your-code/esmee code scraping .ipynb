{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris Banes(chrisbanes)',\n",
       " 'Yangshun Tay(yangshun)',\n",
       " 'Franck Nijhof(frenck)',\n",
       " 'Micah Lee(micahflee)',\n",
       " 'Ana Schwendler(anaschwendler)',\n",
       " 'Alon Zakai(kripken)',\n",
       " 'Paulus Schoutsen(balloob)',\n",
       " 'Asim Aslam(asim)',\n",
       " 'Samuel Colvin(samuelcolvin)',\n",
       " 'CrazyMax(crazy-max)',\n",
       " 'Mr.doob(mrdoob)',\n",
       " 'Nico Schlömer(nschloe)',\n",
       " '文翼(wenzhixin)',\n",
       " 'Miek Gieben(miekg)',\n",
       " 'Henrik Rydgård(hrydgard)',\n",
       " 'Tanner(tanner0101)',\n",
       " 'Kyle Roach(iRoachie)',\n",
       " 'Saúl Ibarra Corretgé(saghul)',\n",
       " 'James Agnew(jamesagnew)',\n",
       " 'bluss(bluss)',\n",
       " 'Rob Rix(robrix)',\n",
       " 'Bo-Yi Wu(appleboy)',\n",
       " 'Sascha Grunert(saschagrunert)',\n",
       " 'Cezar Augusto(cezaraugusto)',\n",
       " 'Teppei Fukuda(knqyf263)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n",
    "html = requests.get(url).content\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "#1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "#h1 class=\"h3 lh-condensed\n",
    "tags = ['h1','p']\n",
    "\n",
    "#2.  Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "text_raw = [element.text for element in soup.find_all(tags)][2:] # used teh range to delete the title rows \n",
    "text_names= text_raw[::3] # jumping with 3 to only select the names \n",
    "text_nick_names= text_raw[1::3] # skip te first one (names) and then jumping with 3 to get the nicknames \n",
    "\n",
    "total_names = list(zip(text_names,text_nick_names)) #list(zip(list_a, list_b))\n",
    "\n",
    "\n",
    "#3. Use string manipulation techniques to replace whitespaces and linebreaks \n",
    "#(i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "\n",
    "total_names2 =[x.strip()+ \"(\" + y.strip()+ \")\" for (x,y) in total_names ]\n",
    "# loop indication: for (x,y) in total_names \n",
    "#it will look at the tubple ellement and teh first = x and teh seccond = y \n",
    "#action indication: [x.strip()+ \"(\" + y.strip()+ \")\" \n",
    "# it will strip x and add in to the list \n",
    "\n",
    "total_names2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_2 = 'https://github.com/trending/python?since=daily'\n",
    "from bs4 import BeautifulSoup\n",
    "html = requests.get(url_2).content\n",
    "soup2 = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-faf506b57fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(re.findall(pattern, text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames_raw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#itemDict = {item[0]: item[1:] for item in items} ( from list of list to dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-faf506b57fce>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(re.findall(pattern, text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames_raw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#itemDict = {item[0]: item[1:] for item in items} ( from list of list to dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#<h1 class=\"h3 lh-condensed\">\n",
    "tags = ['h1']\n",
    "names_raw = [element.text for element in soup2.find_all(tags)][1::] # used teh range to delete the title rows \n",
    "names_raw\n",
    "pattern = \"[^\\n/, ]+\"\n",
    "#print(re.findall(pattern, text))\n",
    "\n",
    "names =[re.findall(pattern, x) for x in names_raw]\n",
    "names\n",
    "#itemDict = {item[0]: item[1:] for item in items} ( from list of list to dict)\n",
    "\n",
    "names_dict = {item[0]: item[1:] for item in names}\n",
    "# item[1:] is generic, it works also on sublkist with more items \n",
    "names_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html = requests.get(url_3).content\n",
    "soup3 = BeautifulSoup(html, \"html.parser\")\n",
    "soup3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<a href=\n",
    "\n",
    "\"/wiki/File:Walt_Disney_envelope_ca._1921.jpg\" class=\"image\">\n",
    "<img alt=\"\"src=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg\" \n",
    "decoding=\"async\" width=\"220\" height=\"152\" class=\"thumbimage\" \n",
    "srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/330px-Walt_Disney_envelope_ca._1921.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/440px-Walt_Disney_envelope_ca._1921.jpg 2x\" \n",
    "data-file-width=\"1576\" data-file-height=\"1086\">\n",
    "\n",
    "</a>\n",
    "\"\"\"\n",
    "import re \n",
    "# <a href= \".....\"</a>\n",
    "#upload.wikimedia.org/....jpg\n",
    "\n",
    "#tags = ['a'] # is to vage get a to long list \n",
    "#images_raw = [element.text for element in soup3.find_all(tags)]\n",
    "#images_raw \n",
    "\n",
    "# solution found via google: https://www.w3resource.com/python-exercises/web-scraping/web-scraping-exercise-8.php\n",
    "images = soup3.find_all('img', {'src':re.compile('.jpg')})\n",
    "for image in images: \n",
    "    print(image['src']+'\\n')\n",
    "    \n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_4 = 'https://en.wikipedia.org/wiki/Python' \n",
    "html = requests.get(url_4).content\n",
    "soup4 = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "tags = ['a'] # is to vage get a to long list \n",
    "links_raw = [element.text for element in soup4.find_all(tags)]\n",
    "\n",
    "# to remove the empthy strings out of the list \n",
    "links_raw2 = [[x for x in links_raw if x] for el in links_raw]\n",
    "\n",
    "# flattern to remove the list of lists\n",
    "links_flat = [val for sublist in links_raw2  for val in sublist]\n",
    "links_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_5 = 'https://www.fbi.gov/wanted/topten'\n",
    "html = requests.get(url_5).content\n",
    "soup5 = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "\n",
    "tags = ['h3'] # is to vage get a to long list \n",
    "image_title_raw = [element.text for element in soup5.find_all(tags)]\n",
    "\n",
    "pattern = \"[^\\n]+\"\n",
    "image_title =[re.findall(pattern, x) for x in image_title_raw]\n",
    "\n",
    "\n",
    "image_title_flat = [val for sublist in image_title  for val in sublist]\n",
    "image_title_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = requests.get(url_7).content\n",
    "soup7 = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earthquake2019-10-15\\xa0\\xa0\\xa017:52:05.41hr 01min ago43.66\\xa0N\\xa0\\xa011.93\\xa0E\\xa0\\xa05ML2.7\\xa0CENTRAL ITALY2019-10-15 18:10',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa017:35:59.41hr 18min ago19.80\\xa0S\\xa0\\xa0173.49\\xa0W\\xa0\\xa010mb4.9\\xa0TONGA2019-10-15 18:40',\n",
       " '5IIIearthquake2019-10-15\\xa0\\xa0\\xa017:24:51.41hr 29min ago36.61\\xa0N\\xa0\\xa021.69\\xa0E\\xa0\\xa010ML3.8\\xa0SOUTHERN GREECE2019-10-15 18:00',\n",
       " 'Fearthquake2019-10-15\\xa0\\xa0\\xa017:23:49.81hr 30min ago38.37\\xa0N\\xa0\\xa021.85\\xa0E\\xa0\\xa00ML2.9\\xa0GREECE2019-10-15 17:28',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa017:17:07.61hr 36min ago7.35\\xa0S\\xa0\\xa0129.19\\xa0E\\xa0\\xa0139M 4.3\\xa0KEPULAUAN BABAR, INDONESIA2019-10-15 18:34',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa017:06:51.01hr 47min ago19.69\\xa0N\\xa0\\xa0154.92\\xa0W\\xa0\\xa043Md2.3\\xa0HAWAII REGION, HAWAII2019-10-15 17:10',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa016:59:29.41hr 54min ago19.22\\xa0N\\xa0\\xa0155.48\\xa0W\\xa0\\xa09Ml2.1\\xa0ISLAND OF HAWAII, HAWAII2019-10-15 17:05',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa016:40:56.12hr 13min ago56.36\\xa0N\\xa0\\xa0148.58\\xa0W\\xa0\\xa010mb4.2\\xa0GULF OF ALASKA2019-10-15 18:18',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa016:18:36.12hr 35min ago35.79\\xa0N\\xa0\\xa0117.77\\xa0W\\xa0\\xa05Ml2.0\\xa0SOUTHERN CALIFORNIA2019-10-15 16:20',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa015:56:19.02hr 57min ago0.58\\xa0N\\xa0\\xa0126.10\\xa0E\\xa0\\xa010 M3.8\\xa0MOLUCCA SEA2019-10-15 16:07',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa015:50:44.03hr 03min ago28.52\\xa0S\\xa0\\xa071.25\\xa0W\\xa0\\xa029ML3.6\\xa0ATACAMA, CHILE2019-10-15 16:09',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa015:37:57.53hr 16min ago5.47\\xa0S\\xa0\\xa0151.96\\xa0E\\xa0\\xa040mb4.7\\xa0NEW BRITAIN REGION, P.N.G.2019-10-15 18:27',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa015:25:02.63hr 29min ago19.24\\xa0N\\xa0\\xa0155.48\\xa0W\\xa0\\xa0-1Ml2.3\\xa0ISLAND OF HAWAII, HAWAII2019-10-15 15:30',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa015:06:28.93hr 47min ago69.60\\xa0N\\xa0\\xa0144.45\\xa0W\\xa0\\xa010ML2.6\\xa0NORTHERN ALASKA2019-10-15 15:51',\n",
       " \"3IIearthquake2019-10-15\\xa0\\xa0\\xa014:57:25.33hr 56min ago41.98\\xa0N\\xa0\\xa043.10\\xa0E\\xa0\\xa01ML4.4\\xa0GEORGIA (SAK'ART'VELO)2019-10-15 15:26\",\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa014:54:31.33hr 59min ago38.31\\xa0N\\xa0\\xa040.99\\xa0E\\xa0\\xa07ML2.3\\xa0EASTERN TURKEY2019-10-15 15:27',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa014:42:05.84hr 11min ago57.05\\xa0S\\xa0\\xa025.18\\xa0W\\xa0\\xa030mb4.9\\xa0SOUTH SANDWICH ISLANDS REGION2019-10-15 17:00',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa014:39:04.34hr 14min ago45.89\\xa0N\\xa0\\xa07.04\\xa0E\\xa0\\xa04ML1.5\\xa0NORTHERN ITALY2019-10-15 15:56',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa014:16:41.04hr 37min ago30.42\\xa0S\\xa0\\xa071.31\\xa0W\\xa0\\xa024ML3.0\\xa0COQUIMBO, CHILE2019-10-15 14:32',\n",
       " 'earthquake2019-10-15\\xa0\\xa0\\xa014:10:30.04hr 43min ago9.64\\xa0N\\xa0\\xa084.82\\xa0W\\xa0\\xa022 M3.2\\xa0COSTA RICA2019-10-15 15:50']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "tags = ['tr'] # is to vage get a to long list # td is precieser maar het is handiger de per rij te selecteren \n",
    "earthquakes_raw = [element.text for element in soup7.find_all(tags)][14:34]\n",
    "earthquakes_raw\n",
    "\n",
    "#pattern_tatitude = \"[^\\n]+\" ago43.66\\xa0N\\\n",
    "#pattern_tongtitude = \\xa011.93\\xa0E\\\n",
    "#import re\n",
    "#pattern_7 = \"[^:]+\"\n",
    "#earthquakes =[re.findall(pattern, x) for x in earthquakes_raw]\n",
    "#earthquakes\n",
    "\n",
    "earthquakes_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head><meta charset=\"utf-8\"/><meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/><meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/><title>Hackevents - All hackathons in one place</title><link href=\"/img/favicons/favicon.png\" rel=\"shortcut icon\"/><!--THIS IS DYNAMICALLY FILLED--><meta content=\"website\" property=\"og:type\"/><meta content=\"https://hackevents.co/\" property=\"og:url\"/><meta content=\"Hackevents - All hackathons in one place\" property=\"og:title\"/><meta content=\"Find hackathons anywhere in the world\" property=\"og:description\"/><meta content=\"https://hackevents.co/img/placeholder.jpg\" property=\"og:image\"/><meta content=\"https://hackevents.co/img/placeholder.jpg\" property=\"og:image:secure_url\"/><meta content=\"Hackevents - All hackathons in one place Image\" property=\"og:image:alt\"/><meta content=\"summary_large_image\" name=\"twitter:card\"/><meta name=\"twitter:domain\" value=\"hackevents.co\"/><meta name=\"twitter:title\" value=\"Hackevents - All hackathons in one place\"/><meta name=\"twitter:description\" value=\"Find hackathons anywhere in the world\"/><meta content=\"https://hackevents.co/img/placeholder.jpg\" name=\"twitter:image\"/><meta name=\"twitter:url\" value=\"https://hackevents.co/\"/><meta name=\"twitter:label1\" value=\"Visit Hackevents\"/><meta name=\"twitter:data1\" value=\"hackevents.co\"/><meta name=\"twitter:label2\" value=\"Submit your event\"/><meta name=\"twitter:data2\" value=\"hackevents.co/submit\"/><link href=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css\" rel=\"stylesheet\" type=\"text/css\"/><script src=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js\"></script><script>window.addEventListener(\"load\", function(){\n",
       "    window.cookieconsent.initialise({\n",
       "        \"palette\": {\n",
       "            \"popup\": {\n",
       "                \"background\": \"#eeeeee\",\n",
       "                \"text\": \"#000000\"\n",
       "            },\n",
       "            \"button\": {\n",
       "                \"background\": \"#000000\",\n",
       "                \"text\": \"#ffffff\"\n",
       "            }\n",
       "        },\n",
       "        \"type\": \"opt-in\",\n",
       "        \"revokable\": true,\n",
       "        \"revokeBtn\":'<div></div>',\n",
       "        \"onStatusChange\": function (status) {\n",
       "            if(status === \"allow\"){\n",
       "                //runCookieContainingFunctions()\n",
       "            }\n",
       "\n",
       "        },\n",
       "        \"onInitialise\": function(status) {\n",
       "            if (status === \"allow\") {\n",
       "                //runCookieContainingFunctions()\n",
       "            }\n",
       "        },\n",
       "        law: {\n",
       "            regionalLaw: false\n",
       "        },\n",
       "        location: true,\n",
       "        \"content\": {\n",
       "            \"message\": \"We use cookies to make interactions with our websites and services easy and meaningful, to better understand how they are used and to tailor advertising. You can read more and make your cookie choices here. By continuing to use this site you are giving us your consent to do this.\",\n",
       "            \"dismiss\": \"Decline\",\n",
       "            \"allow\": \"Allow cookies\",\n",
       "            \"link\": \"Data Privacy\",\n",
       "            \"href\": \"/legal/data-privacy\"\n",
       "        }\n",
       "    })});\n",
       "</script><script src=\"https://www.gstatic.com/firebasejs/5.5.9/firebase.js\"></script><script>var config = {\n",
       "    apiKey: \"AIzaSyDV4CgBqZMTr-TGeoJjDJhA0mNUrpXoux0\",\n",
       "    authDomain: \"hackevents-prod.firebaseapp.com\",\n",
       "    databaseURL: \"https://hackevents-prod.firebaseio.com\",\n",
       "    projectId: \"hackevents-prod\",\n",
       "    storageBucket: \"hackevents-prod.appspot.com\",\n",
       "    messagingSenderId: \"674040275473\"\n",
       "};\n",
       "firebase.initializeApp(config);</script><script>// Initialize Firebase\n",
       "console.log(\"FIrebase initialized\");\n",
       "\n",
       "firebase.auth().onAuthStateChanged(function (user) {\n",
       "    if (user) {\n",
       "        return console.log(\"Signed in\");\n",
       "    }\n",
       "    firebase.auth().signInAnonymously()\n",
       "});\n",
       "\n",
       "</script><script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=UA-58568758-1\"></script><script>window.dataLayer = window.dataLayer || [];\n",
       "function gtag(){dataLayer.push(arguments);}\n",
       "gtag('js', new Date());\n",
       "gtag('config', 'UA-58568758-1');\n",
       "\n",
       "</script><link crossorigin=\"anonymous\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css\" integrity=\"sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO\" rel=\"stylesheet\"/><script src=\"https://code.jquery.com/jquery-3.3.1.min.js\"></script><script crossorigin=\"anonymous\" integrity=\"sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49\" src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js\"></script><script crossorigin=\"anonymous\" integrity=\"sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy\" src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js\"></script><link crossorigin=\"anonymous\" href=\"https://use.fontawesome.com/releases/v5.3.1/css/all.css\" integrity=\"sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU\" rel=\"stylesheet\"/><link href=\"/css/hackevents.css\" rel=\"stylesheet\"/><link href=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css\" rel=\"stylesheet\" type=\"text/css\"/><script src=\"https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment.min.js\" type=\"text/javascript\"></script><!--submission form date picker--><script src=\"https://cdnjs.cloudflare.com/ajax/libs/tempusdominus-bootstrap-4/5.0.0-alpha14/js/tempusdominus-bootstrap-4.min.js\" type=\"text/javascript\"></script><link href=\"https://cdnjs.cloudflare.com/ajax/libs/tempusdominus-bootstrap-4/5.0.0-alpha14/css/tempusdominus-bootstrap-4.min.css\" rel=\"stylesheet\"/><link href=\"https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.6-rc.0/css/select2.min.css\" rel=\"stylesheet\"/><script src=\"https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.6-rc.0/js/select2.min.js\"></script><link href=\"https://cdnjs.cloudflare.com/ajax/libs/select2-bootstrap-theme/0.1.0-beta.10/select2-bootstrap.min.css\" rel=\"stylesheet\"/></head><body></body></html><div class=\"container-fluid\" style=\"background: url(/img/header.jpg);background-repeat: no-repeat;background-size: cover; height: 60vh\"><nav class=\"navbar navbar-expand-lg navbar-light bg-light\" style=\"background-color: rgba(0,0,0,0) !important;\"><div class=\"container\"><a class=\"navbar-brand\" href=\"/\"><img alt=\"Hackevents Logo\" class=\"d-inline-block align-top\" height=\"18\" src=\"/img/logos/HACKEVENTS_white.svg\" width=\"130\"/></a><button aria-controls=\"navbarNavAltMarkup\" aria-expanded=\"false\" aria-label=\"Toggle navigation\" class=\"navbar-toggler\" data-target=\"#navbarNavAltMarkup\" data-toggle=\"collapse\" style=\"border:0;\" type=\"button\"><span class=\"navbar-toggler-icon\"></span></button><div class=\"collapse navbar-collapse\" id=\"navbarNavAltMarkup\"><div class=\"navbar-nav ml-auto\"><a class=\"nav-item nav-link active\" href=\"/search/anything/anywhere/anytime\" style=\"margin-right: 30px; color: white;\">Browse Hackathons</a></div><div class=\"navbar-nav\"><a class=\"nav-item nav-link active\" href=\"/submit\" style=\"margin-right: 30px; color: white;\">Submit Event</a></div></div></div></nav><div class=\"container\" style=\"margin-top:10vh\"><div class=\"row\" style=\"color:white\"><div class=\"col-12\"><h1 class=\"text-center\" style=\"font-size:50px\">All hackathons in one place</h1><h2 class=\"text-center\" style=\"font-size:22px\">Find hackathons anywhere in the world</h2></div></div></div></div><div class=\"container rounded box-shadow\" style=\"background-color:white;margin-top:-100px;\"><div class=\"row\"><div class=\"col-12\"><h3 class=\"text-center\" style=\"font-size:30px; margin-top: 24px; margin-bottom: 4px;\">Choose from <b>1</b> upcoming hackathons <b>around the world</b></h3><!--in #{numberOfDistinctCities} cities--></div></div><div class=\"row\"><div class=\"col-12 col-sm-8 offset-sm-2\"><hr/></div></div><form action=\"/searchResolver\" class=\"row\" method=\"get\" style=\"margin-bottom:24px\"><div class=\"col-12 col-sm-5\" style=\"margin-top:8px;\"><h4 style=\"font-size:18px;\">Looking for</h4><input class=\"form-control\" name=\"searchTitle\" placeholder=\"Anything\" style=\"width:100%\"/></div><div class=\"col-12 col-sm-5\" style=\"margin-top:8px;\"><h4 style=\"font-size:18px;\">Best place would be</h4><input class=\"form-control\" name=\"searchPlace\" placeholder=\"Anywhere\" style=\"width:100%\"/></div><div class=\"col-12 col-sm-2\" style=\"margin-top:8px;\"><button class=\"btn btn-outline-danger\" style=\"width:100%; height:70px;\" type=\"submit\">Search</button></div></form></div><div class=\"container\"><div class=\"row\"><div class=\"col-12 text-center\" style=\"margin-top:100px;margin-bottom:50px\"><h2>Upcoming hackathons</h2></div></div><div class=\"row\"><div class=\"col-12 col-sm-4\"><a href=\"/event/lRFgffzuhdHfABp5r9sJ-oerlikon-digital-hub-hackathon\"><div class=\"card box-shadow\" style=\"width: 100%;margin-bottom: 16px;\"><!--img.card-img-top(id=imgId alt=hackathon.title height=\"200px\")--><img alt=\"Oerlikon Digital Hub Hackathon\" class=\"card-img-top\" height=\"200px\" src=\"https://storage.googleapis.com/hackevents-prod.appspot.com/hackathons%2Fimages%2FcheTopz6muhqvPH57ZnNWi7Uy8r2%2Fdigital-hub_hackathon_logo_red_rgb.png?GoogleAccessId=firebase-adminsdk-w04bv%40hackevents-prod.iam.gserviceaccount.com&amp;Expires=13569465600&amp;Signature=YrZbFy43h9VhMhJ%2FsGiR7Ym8esAHiO80yyWCIX2BzDLWTZcIQUTLd7CNWJSQbzjexoAR7WXbRLVwClZJ%2BXG1Y2BRM1uOKOpzoYX4IhE7dV7uXJWBcRQd8ESHOT9NUcN4pDV2ue3B9c7gF7v8qDZCI3iOsIJnXkeDfH%2FGxJBUnLRXwLHGA6I%2BgbBk%2B7xcXi73v81Deis7myM%2BoDLySZmIeMa13kK%2BLr3PsKZHWBv%2BVnzeoYX40XgQKv6OYIObYSetJUfpPwvoOpylHJTdNfnPD5C9el7NSOzf%2FCzFyy6XPd1QZ0RJXmDmdjt5GtzOdjCt%2F0ln7MAq1JfLBnbh1Y%2FLfA%3D%3D\"/><!--include __firebaseStorageImageLoader--><div class=\"card-body\" style=\"color:black\"><h5 class=\"card-title\">Oerlikon Digital Hub Hackathon</h5><p class=\"card-text\"><i class=\"fas fa-calendar-alt\"></i> 11/8/2019\n",
       "</p></div></div></a></div></div></div>\n",
       "<i class=\"fas fa-map-marker-alt\"></i> Munich, Feldkirchen, Germany<div class=\"row\" style=\"margin-top:75px;margin-bottom:75px;\"><div class=\"col-12 text-center\"><a href=\"/search/anything/anywhere/anytime\">See more &gt;</a></div></div><div class=\"container-fluid\" style=\"margin-top:75px;background-color:#4963A3;color:white;height:150px;\"><div class=\"container\"><div class=\"row\" style=\"height:100%\"><div class=\"col-12 col-sm-10 my-auto\"><h3 style=\"font-size:38px\">Want to add a hackathon?</h3></div><div class=\"col-12 col-sm-2 my-auto\"><a class=\"btn btn-outline-light\" href=\"/submit\" style=\"width:100%;\">Submit event</a></div></div></div></div><div class=\"container\"><div class=\"row\"><div class=\"col-12 text-center\" style=\"margin-top:100px;margin-bottom:50px\"><h2>More upcoming hackathons</h2></div></div><div class=\"row\"></div><div class=\"row\" style=\"margin-top:75px;margin-bottom:75px;\"><div class=\"col-12 text-center\"><a href=\"/search/anything/anywhere/anytime\">See more &gt;</a></div></div></div><div class=\"container-fluid\" style=\"background-color:#A34D49;color:white;height:150px;\"><div class=\"container\"><form action=\"https://hackevents.us11.list-manage.com/subscribe/post?u=12d2135049d488583f81df765&amp;id=f47626f04f\" method=\"post\" name=\"mc-embedded-subscribe-form\" novalidate=\"\" target=\"_blank\"><div class=\"row\" style=\"height:100%\"><div class=\"col-12 col-sm-7 my-auto\"><h3 style=\"font-size:38px\">Newsletter sign up</h3></div><div class=\"col-12 col-sm-3 my-auto\"><input class=\"form-control\" name=\"EMAIL\" placeholder=\"email address\" required=\"\" style=\"width:100%\" type=\"email\" value=\"\"/></div><div class=\"col-12 col-sm-2 my-auto\"><!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups--><div aria-hidden=\"true\" style=\"position: absolute; left: -5000px;\"><input name=\"b_12d2135049d488583f81df765_f47626f04f\" tabindex=\"-1\" type=\"text\" value=\"\"/></div><button class=\"btn btn-outline-light\" name=\"subscribe\" style=\"width:100%\" type=\"submit\" value=\"Sign up\">Sign up</button></div></div></form></div></div><div class=\"container-fluid footer\"><div class=\"container\" style=\"padding-top:25px; padding-bottom:50px;\"><div class=\"row\"><div class=\"col-6 col-sm-3\"><p class=\"footerTitle\"><img alt=\"Hackevents Logo\" height=\"15\" src=\"/img/logos/HACKEVENTS_white.svg\" style=\"margin-top:8px;\"/></p><p class=\"footerEntry\">a product of Hackerbay</p><p class=\"footerEntry\"><a href=\"/legal/imprint\">Imprint</a></p><p class=\"footerEntry p9\" style=\"padding-top:10px\">© <script>document.write(new Date().getFullYear())</script> Singularity Technologies GmbH.<br/>All rights reserved.</p></div><div class=\"col-6 col-sm-3\"><p class=\"footerTitle\">FOR ORGANIZERS</p><p class=\"footerEntry\"><a href=\"/submit\">Submit event</a></p></div><div class=\"col-6 col-sm-3\"><p class=\"footerTitle\">FOR EU HACKERS</p><p class=\"footerEntry\"><a href=\"/search/anything/Germany/anytime\">Hackathons in Germany</a></p><p class=\"footerEntry\"><a href=\"/search/anything/Berlin/anytime\">Hackathons in Berlin</a></p><p class=\"footerEntry\"><a href=\"/search/anything/France/anytime\">Hackathons in France</a></p><p class=\"footerEntry\"><a href=\"/search/anything/London/anytime\">Hackathons in London</a></p></div><div class=\"col-6 col-sm-3\"><p class=\"footerTitle\">FOR US HACKERS</p><p class=\"footerEntry\"><a href=\"/search/anything/San Francisco/anytime\">Hackathons in San Francisco</a></p><p class=\"footerEntry\"><a href=\"/search/anything/New York/anytime\">Hackathons in New York</a></p><p class=\"footerEntry\"><a href=\"/search/anything/Canada/anytime\">Hackathons in Canada</a></p><p class=\"footerEntry\"><a href=\"/search/anything/United States/anytime\">Hackathons in United States</a></p></div></div></div></div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_8 ='https://hackevents.co/hackathons'\n",
    "html = requests.get(url_8).content\n",
    "soup8 = BeautifulSoup(html, \"html.parser\")\n",
    "soup8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#date, days, title, city, country of next 25 \n",
    "\n",
    "#??? there is only one hackaton on the page, also after searching for more in the menue option above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
